# General Object Detection

([Jiao et al., 2019](https://arxiv.org/abs/1907.09408)) A Survey of Deep Learning-based Object Detection

# Few-shot Computer Vision

([Tian et al., 2020](https://arxiv.org/abs/2003.11539)) Rethinking few-shot image classification: a good embedding is all you need?

# Image Understanding and Concept Learning

Notable authors: [Joshua B. Tenenbaum](https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en)

([Chen et al., 2022](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890708.pdf)) Unsupervised segmentation in real-world images via spelke object inference
([Friedman et al., 2022](https://escholarship.org/uc/item/7qc5j87f)) Benchmarking mid-level vision with texture-defined 3D objects
([Sutherland et al., 2022](https://arxiv.org/abs/2203.02554)) Building 3D Generative Models from Minimal Data
([Sablé-Meyer et al., 2022](https://psyarxiv.com/28mg4)) A language of thought for the mental representation of geometric shapes

# Program Induction

Notable authors: [Joshua B. Tenenbaum](https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en)

([Ellis et al., 2018](https://proceedings.neurips.cc/paper/2018/file/7aa685b3b1dc1d6780bf36f7340078c9-Paper.pdf)) Library Learning for Neurally-Guided Bayesian Program Induction
([Ellis et al., 2020](https://arxiv.org/abs/2006.08381)) Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning
([Ellis et al., 2022](https://www.nature.com/articles/s41467-022-32012-w)) Synthesizing theories of human language with Bayesian program induction
([Rule et al., 2022](https://escholarship.org/uc/item/9q79v2h8)) Learning as Programming: Modeling Efficient Search in Human Concept Learning
([Shi et al., 2022](https://arxiv.org/abs/2211.11033)) On the Complexity of Bayesian Generalization

# Neuro-Symbolic Models

([Liang et al., 2022](https://arxiv.org/abs/2206.01829)) (#OOD) Drawing out of Distribution with Neuro-Symbolic Generative Models

# Wide Generalization and Consciousness

Notable authors: [Stanislas Dehaene](https://scholar.google.com/citations?hl=en&user=2Dd5uoIAAAAJ&view_op=list_works&sortby=pubdate), [Karl Friston](https://scholar.google.co.uk/citations?user=q_4u0aoAAAAJ&hl=en), [Yair Lakretz](https://scholar.google.co.il/citations?hl=fr&user=cNnJ5YUAAAAJ&view_op=list_works&sortby=pubdate), [Joshua B. Tenenbaum](https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en)

([Dehaene and Dehaene-Lambertz, 2016](https://www.unicog.org/publications/nn.4369.pdf)) Is the brain prewired for letters?
([Michel et al., 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568255/pdf/nihms-1027336.pdf)) Opportunities and challenges for a maturing science of consciousness
([Demertzi et al., 2019](https://www.science.org/doi/full/10.1126/sciadv.aat7603)) Human consciousness is supported by dynamic complex patterns of brain signal coordination
([Lakretz et al., 2019](https://arxiv.org/abs/1903.07435)) The emergence of number and syntax units in LSTM language models
([Melloni et al., 2019](https://www.esforum.de/publications/PDFs/sfr27/SFR27_17_Melloni%20et%20al.pdf)) Computation and its neural implementation in human cognition
([Dasgupta et al., 2020](https://www.biorxiv.org/content/10.1101/644534v2.full.pdf)) A theory of learning to infer
([Zhu et al., 2020](https://www.sciencedirect.com/science/article/pii/S2095809920300345)) Dark, beyond deep: A paradigm shift to cognitive ai with humanlike common sense
([Shiffrin et al., 2020](https://www.pnas.org/doi/full/10.1073/pnas.1912340117)) The brain produces mind by modeling
([Schrimpf et al., 2020](https://www.biorxiv.org/content/biorxiv/early/2020/06/27/2020.06.26.174482.full.pdf)) Artificial neural networks accurately predict language processing in the brain
([Maheu et al., 2020](https://www.biorxiv.org/content/10.1101/2020.02.06.937706v2.full.pdf)) Rational arbitration between statistics and rules in human sequence learning
([Dehaene, 2020](file:///C:/Users/Oleg/Downloads/Howwelearnthenewscienceofeducationandthebrain.pdf)) How we learn: The new science of education and the brain
([Mashour et al., 2020](https://www.sciencedirect.com/science/article/pii/S0896627320300520)) Conscious Processing and the Global Neuronal Workspace Hypothesis
([Lakretz et al., 2020](https://www.mdpi.com/1099-4300/22/4/446/pdf)) What limits our capacity to process nested long-range dependencies in sentence comprehension?
([Gui et al., 2020](https://www.researchgate.net/publication/341627484_Assessing_the_depth_of_language_processing_in_patients_with_disorders_of_consciousness)) Assessing the depth of language processing in patients with disorders of consciousness
([Schrimpf et al., 2021](https://www.pnas.org/doi/pdf/10.1073/pnas.2105646118)) The neural architecture of language: Integrative modeling converges on predictive processing
([Friston et al., 2021](https://www.sciencedirect.com/science/article/pii/S0893608021003610)) World model learning and inference
([Dehaene et al., 2021](https://library.oapen.org/bitstream/handle/20.500.12657/47279/9783030541736.pdf?sequence=1#page=48)) What is consciousness, and could machines have it?
([Lakretz et al., 2021](https://arxiv.org/abs/2101.02258)) Can RNNs learn Recursive Nested Subject-Verb Agreements?
([Dehaene, 2021](https://www.goodreads.com/book/show/46064083-how-we-learn)) (BOOK) How we learn: Why brains learn better than any machine... for now
([Woolnough et al., 2021](https://www.biorxiv.org/content/biorxiv/early/2020/02/19/2020.02.18.955039.full.pdf)) Spatiotemporal dynamics of orthographic and lexical processing in the ventral visual pathway
([Lakretz et al., 2021](https://arxiv.org/abs/2006.11098)) Mechanisms for Handling Nested Dependencies in Neural-Network Language Models and Humans
([Hong et al., 2021](https://proceedings.neurips.cc/paper/2021/file/918f5cd5a5c0d48671d4d4fc54bab2e9-Paper.pdf)) Ptr: A benchmark for part-based conceptual, relational, and physical reasoning
([Paulun et al., 2022](https://escholarship.org/uc/item/2689k3rx)) Joint online inference of material properties and object shape
([Tomov et al., 2022](https://www.biorxiv.org/content/biorxiv/early/2022/06/16/2022.06.14.496001.full.pdf)) The Neural Architecture of Theory-based Reinforcement Learning
([Tessler et al., 2022](https://www.researchgate.net/publication/357733767_Logic_Probability_and_Pragmatics_in_Syllogistic_Reasoning)) Logic, Probability, and Pragmatics in Syllogistic Reasoning
([Czégel et al., 2022](https://onlinelibrary.wiley.com/doi/pdf/10.1002/bies.202100255)) Bayes and Darwin: How replicator populations implement Bayesian computations
([De Freitas et al., 2022](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4220026)) What Would It Mean for a Machine to Have a Self?
([Zhi-Xuan et al., 2022](https://arxiv.org/abs/2208.02914)) Solving the Baby Intuitions Benchmark with a Hierarchically Bayesian Theory of Mind
([Chen et al., 2022](https://arxiv.org/abs/2205.01089)) ComPhy: Compositional Physical Reasoning of Objects and Events from Videos
([Zhang et al., 2022](https://www.jneurosci.org/content/jneuro/42/5/850.full.pdf)) Working Memory for Spatial Sequences: Developmental and Evolutionary Factors in Encoding Ordinal and Relational Structures
([Bellet et al., 2022](https://www.biorxiv.org/content/biorxiv/early/2022/09/30/2021.10.04.463064.full.pdf)) Spontaneously emerging internal models of visual sequences combine abstract and event-specific information in the prefrontal cortex
([Zacharopoulos et al., 2022](https://www.biorxiv.org/content/biorxiv/early/2022/07/10/2022.07.08.499161.full.pdf)) Disentangling Hierarchical and Sequential Computations during Sentence Processing
([Srivastava et al., 2022](https://arxiv.org/abs/2206.04615)) Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
([Dehaene et al., 2022](https://www.unicog.org/publications/Dehaene%20TICS%202022%20final%20proofs.pdf)) Symbols and mental programs: a hypothesis about human singularity
([Lakretz et al., 2022](https://arxiv.org/abs/2110.07240)) Causal Transformers Perform Below Chance on Recursive Nested Constructions, Unlike Humans
([Lakretz et al., 2022](https://aclanthology.org/2022.coling-1.285/)) Can Transformers Process Recursive Nested Constructions, Like Humans?

# MIT research: Perception and Scene Understanding for Embodied AI, Energy-Based Models

Notable authors: [Yilun Du](https://scholar.google.com/citations?user=GRMMc_MAAAAJ&hl=en) ([blog](https://yilundu.github.io/)), [Shuang Li](https://scholar.google.com/citations?user=auA1nNAAAAAJ&hl=en) ([page](https://people.csail.mit.edu/lishuang/)), [Joshua B. Tenenbaum](https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en)

([Du et al., 2020](https://arxiv.org/abs/2007.12348)) Unsupervised discovery of 3d physical objects from video
([Jara-Ettinger et al., 2021](http://130.132.167.20/docs/2020/cogpsych_NUC.pdf)) The naive utility calculus as a unified, quantitative framework for action understanding
([Alet et al., 2021](http://proceedings.mlr.press/v139/alet21a/alet21a.pdf)) A large-scale benchmark for few-shot program induction and synthesis
([Tsividis et al., 2021](https://arxiv.org/abs/2107.12544)) Human-level reinforcement learning through theory-based modeling, exploration, and planning
([Wu et al., 2021](https://openreview.net/forum?id=EfgNF5-ZAjM)) STAR: A Benchmark for Situated Reasoning in Real-World Videos
([Luo et al., 2021](https://openreview.net/forum?id=HRF6T1SsyDn)) On the Expressiveness and Learning of Relational Neural Networks on Hypergraphs
([Gerstenberg et al., 2021](https://dspace.mit.edu/bitstream/handle/1721.1/138370/Gerstenberg%20et%20al.%20-%20A%20counterfactual%20simulation%20model%20of%20causal%20judgment.pdf?sequence=2&isAllowed=y)) A counterfactual simulation model of causal judgments for physical events
([Du et al., 2020](https://arxiv.org/abs/2012.09790)) ([post](https://yilundu.github.io/nerflow/)) Neural Radiance Flow for 4D View Synthesis and Video Processing 
([Du et al., 2021](https://arxiv.org/abs/2111.06387)) Learning Signal-Agnostic Manifolds of Neural Fields
([Sitzmann et al., 2021](https://proceedings.neurips.cc/paper/2021/file/a11ce019e96a4c60832eadd755a17a58-Paper.pdf)) Light field networks: Neural scene representations with single-evaluation rendering
([Nye et al., 2021](https://proceedings.neurips.cc/paper/2021/file/d3e2e8f631bd9336ed25b8162aef8782-Paper.pdf)) Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning
([Ding et al., 2021](https://proceedings.neurips.cc/paper/2021/file/07845cd9aefa6cde3f8926d25138a3a2-Paper.pdf)) Dynamic visual reasoning by learning differentiable physics models from video and language
([Gothoskar et al., 2021](https://proceedings.neurips.cc/paper/2021/file/4fc66104f8ada6257fa55f29a2a567c7-Paper.pdf)) 3DP3: 3D Scene Perception via Probabilistic Programming
([Simeonov et al., 2022](https://arxiv.org/abs/2112.05124)) Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation
([Hong et al., 2022](https://arxiv.org/abs/2207.06403)) 3D Concept Grounding on Neural Fields
([Sharma et al., 2022](https://arxiv.org/abs/2207.11232)) Seeing 3D Objects in a Single Image via Self-Supervised Static-Dynamic Disentanglement
([Fu et al., 2022](https://arxiv.org/abs/2208.01014)) Robust Change Detection Based on Neural Descriptor Fields
([Simeonov et al., 2022](https://arxiv.org/abs/2211.09786)) SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields

https://energy-based-model.github.io/Energy-based-Model-MIT/ 
([Du and Mordatch, 2019](https://arxiv.org/abs/1903.08689)) Implicit Generation and Generalization in Energy-Based Models
([Du et al., 2019](https://arxiv.org/abs/1909.06878)) (#RL) Model Based Planning with Energy Based Models
([Du et al., 2020](https://arxiv.org/abs/2004.13167)) Energy-based models for atomic-resolution protein conformations
([Du et al., 2020](https://arxiv.org/abs/2004.06030)) (#Comp) Compositional Visual Generation with Energy Based Models
([Li et al., 2020](https://arxiv.org/abs/2011.12216)) (#DataShift) Energy-Based Models for Continual Learning
([Du et al., 2020](https://arxiv.org/abs/2012.01316)) Improved Contrastive Divergence Training of Energy Based Models
([Liu et al., 2021](https://arxiv.org/abs/2111.09297)) (#Comp) Learning to Compose Visual Relations
([Du et al., 2021](https://arxiv.org/abs/2111.03042)) (#Comp #WS) Unsupervised Learning of Compositional Energy Concepts
([Du et al, 2022](https://arxiv.org/abs/2206.15448)) (#Reasoning) Learning Iterative Reasoning through Energy Minimization
([Liu et al., 2022](https://arxiv.org/abs/2206.01714)) (#Diffusion #Comp) Compositional Visual Generation with Composable Diffusion Models

([Du et al., 2021](https://arxiv.org/abs/2105.01060)) Curious Representation Learning for Embodied Intelligence
([Li et al., 2022](https://arxiv.org/abs/2202.01771)) Pre-Trained Language Models for Interactive Decision-Making
([Janner et al., 2022](https://arxiv.org/abs/2205.09991)) (#RL) Planning with Diffusion for Flexible Behavior Synthesis
([Schaeffer et al., 2022](https://arxiv.org/abs/2205.01212)) (#OOD) Streaming Inference for Infinite Non-Stationary Clustering
([Schaeffer et al., 2022](https://proceedings.mlr.press/v162/schaeffer22a/schaeffer22a.pdf)) (#DataShift) Streaming Inference for Infinite Feature Models
([Li et al., 2022](https://arxiv.org/abs/2210.11522)) Composing Ensembles of Pre-trained Models via Iterative Consensus


